{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "RESULTS_PATH = \"../results/tpch/2025-03-05-15H\"\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "meta_results = pd.read_csv(RESULTS_PATH + \"/meta_results.csv\")\n",
    "\n",
    "# Function to count the number of materialized fields from the string representation\n",
    "def count_materialized_fields(s: str):\n",
    "    try:\n",
    "        #TODO Remove this when the bug in perform_load_test is fixed\n",
    "        if s.startswith(\"dict_keys\"):\n",
    "            s_list = s[len(\"dict_keys(\"):-1]\n",
    "            fields = ast.literal_eval(s_list)\n",
    "        else:\n",
    "            # Converts the string representation to a python list\n",
    "            fields = ast.literal_eval(s)\n",
    "        return len(fields)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Create a new column with the number of materialized fields\n",
    "meta_results[\"num_materialized_fields\"] = meta_results[\"Materialization\"].apply(count_materialized_fields)\n",
    "\n",
    "# Exclude rows where Test is full_materialization\n",
    "meta_results_no_full = meta_results[meta_results[\"Test\"] != \"full_materialization\"]\n",
    "\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(meta_results_no_full[\"num_materialized_fields\"], meta_results_no_full[\"Database size\"])\n",
    "plt.xlabel(\"Number of Materialized Fields\")\n",
    "plt.ylabel(\"Database Size\")\n",
    "plt.title(\"Scatter Plot: Number of Materialized Fields vs. Database Size\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(meta_results[\"num_materialized_fields\"], meta_results[\"Time taken\"])\n",
    "plt.xlabel(\"Number of Materialized Fields\")\n",
    "plt.ylabel(\"Time Taken to Materialize\")\n",
    "plt.title(\"Scatter Plot: Number of Materialized Fields vs. Time Taken to Materialize\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = meta_results[meta_results[\"Total query time\"] > 0]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(subset[\"num_materialized_fields\"], subset[\"Total query time\"])\n",
    "plt.xlabel(\"Number of Materialized Fields\")\n",
    "plt.ylabel(\"Total Query Time\")\n",
    "plt.title(\"Scatter Plot: Number of Materialized Fields vs. Total Query Time\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# Define the tests to include\n",
    "tests_to_include = [\n",
    "    \"full_materialization\",\n",
    "    \"schema_based_materialization\",\n",
    "    \"load_based_t0.33\",\n",
    "    \"load_based_t0.5\"\n",
    "]\n",
    "\n",
    "df_filtered_on_tests = meta_results[meta_results[\"Test\"].isin(tests_to_include)]\n",
    "\n",
    "df_filtered_on_tests_and_load = df_filtered_on_tests[(df_filtered_on_tests[\"Query proportion\"] == 3) & (df_filtered_on_tests[\"Majority proportion\"] == 80)]\n",
    "\n",
    "# Pivot the DataFrame so that each Load is an index and each Test is a column with \"Time taken\" as values\n",
    "pivot_df = df_filtered_on_tests_and_load.pivot(index=\"Load\", columns=\"Test\", values=\"Total query time\").sort_index()\n",
    "\n",
    "colors = {\n",
    "    \"full_materialization\": \"#4C72B0\",          # muted blue\n",
    "    \"schema_based_materialization\": \"#55A868\",  # muted green\n",
    "    \"load_based_t0.33\": \"#C44E52\",               # muted red\n",
    "    \"load_based_t0.5\": \"#8172B3\"                 # muted purple\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "loads = pivot_df.index.values  # expected loads: 0, 1, 2, 3, 4\n",
    "x = np.arange(len(loads))        # group positions\n",
    "width = 0.2                    # width of each bar\n",
    "\n",
    "# Loop over each load group\n",
    "for i, load in enumerate(loads):\n",
    "    # Extract the row for the load group and sort by \"Time taken\" (ascending)\n",
    "    group = pivot_df.loc[load]\n",
    "    sorted_group = group.sort_values()  # sorts tests by time taken\n",
    "    # Plot each bar in sorted order (left to right)\n",
    "    for j, test in enumerate(sorted_group.index):\n",
    "        ax.bar(x[i] + j * width, sorted_group[test], width, color=colors[test])\n",
    "\n",
    "# Set x-ticks in the center of each group\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(loads)\n",
    "ax.set_xlabel(\"Load\")\n",
    "ax.set_ylabel(\"Total query time\")\n",
    "ax.set_title(\"Total query time for 3|80\")\n",
    "\n",
    "# Create a custom legend (order as in tests_to_include)\n",
    "legend_handles = [mpatches.Patch(color=colors[test], label=test) for test in tests_to_include]\n",
    "ax.legend(handles=legend_handles)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_filtered_on_tests[\"combination\"] = df_filtered_on_tests[\"Query proportion\"].astype(str) + \"/\" + df_filtered_on_tests[\"Majority proportion\"].astype(str)\n",
    "\n",
    "# First, calculate both the mean and standard deviation (and count) for each group\n",
    "grouped = df_filtered_on_tests.groupby([\"combination\", \"Test\"])[\"Total query time\"] \\\n",
    "    .agg(mean=\"mean\", std=\"std\", count=\"count\").reset_index()\n",
    "\n",
    "# Calculate the standard error of the mean\n",
    "grouped[\"stderr\"] = grouped[\"std\"] / np.sqrt(grouped[\"count\"])\n",
    "\n",
    "# Pivot the DataFrame for means and for standard errors separately\n",
    "pivot_df = grouped.pivot(index=\"combination\", columns=\"Test\", values=\"mean\").sort_index()\n",
    "stderr_df = grouped.pivot(index=\"combination\", columns=\"Test\", values=\"stderr\").sort_index()\n",
    "\n",
    "colors = {\n",
    "    \"full_materialization\": \"#4C72B0\",          # muted blue\n",
    "    \"schema_based_materialization\": \"#55A868\",  # muted green\n",
    "    \"load_based_t0.33\": \"#C44E52\",              # muted red\n",
    "    \"load_based_t0.5\": \"#8172B3\"                # muted purple\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "combinations_sorted = pivot_df.index.values  # These are our x-axis groups\n",
    "x = np.arange(len(combinations_sorted))\n",
    "width = 0.2  # width of each bar\n",
    "\n",
    "# Loop over each combination group\n",
    "for i, comb in enumerate(combinations_sorted):\n",
    "    # Extract the row for the current combination for both mean and uncertainty, drop missing values\n",
    "    group = pivot_df.loc[comb].dropna()\n",
    "    group_stderr = stderr_df.loc[comb].dropna()\n",
    "    # Sort tests by average total query time (ascending)\n",
    "    sorted_group = group.sort_values()\n",
    "    # Reorder uncertainties to match the sorted order\n",
    "    sorted_stderr = group_stderr[sorted_group.index]\n",
    "    # Plot each bar in the sorted order for this combination with error bars\n",
    "    for j, test in enumerate(sorted_group.index):\n",
    "        ax.bar(x[i] + j * width, sorted_group[test], width, color=colors[test],\n",
    "               yerr=sorted_stderr[test], capsize=5)\n",
    "\n",
    "# Set x-ticks in the center of each group\n",
    "# tests_to_include should be defined as a list of tests in the order you want them in the legend\n",
    "ax.set_xticks(x + width * (len(tests_to_include) - 1) / 2)\n",
    "ax.set_xticklabels(combinations_sorted)\n",
    "ax.set_xlabel(\"Query proportion / Majority proportion\")\n",
    "ax.set_ylabel(\"Average Total Query Time\")\n",
    "ax.set_title(\"Average Total Query Time Across Combinations\\n(Each Bar is Averaged Over 5 Instances with Uncertainty)\")\n",
    "ax.grid(True, axis='y')\n",
    "\n",
    "# Create a custom legend (the order is as defined in tests_to_include)\n",
    "legend_handles = [mpatches.Patch(color=colors[test], label=test) for test in tests_to_include]\n",
    "ax.legend(handles=legend_handles)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
