{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from enum import IntEnum\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "import testing.tpch.setup as tpch_setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SCALE_FACTOR = 0.5\n",
    "RESULTS_PATH = f\"{os.curdir}/results/single-queries/tpch/2025-05-10-15H/\"\n",
    "PERCENTILE_THRESHOLD = 0.8  # Used for filtering high-performing materializations\n",
    "\n",
    "# Table size definitions based on scale factor\n",
    "TPCH_TABLE_SIZES = {\n",
    "    \"customer\": int(150000 * SCALE_FACTOR),\n",
    "    \"lineitem\": int(6000000 * SCALE_FACTOR),\n",
    "    \"orders\": int(1500000 * SCALE_FACTOR),\n",
    "    \"part\": int(200000 * SCALE_FACTOR),\n",
    "    \"partsupp\": int(800000 * SCALE_FACTOR),\n",
    "    \"supplier\": int(10000 * SCALE_FACTOR),\n",
    "    \"nation\": 25,  # Not scaled\n",
    "    \"region\": 5    # Not scaled\n",
    "}\n",
    "\n",
    "# Load query definitions\n",
    "QUERIES = tpch_setup.QUERIES\n",
    "\n",
    "# Define join categories\n",
    "\n",
    "\n",
    "class JoinCategory(IntEnum):\n",
    "    \"\"\"Categories for join operations based on table sizes and materialization state\"\"\"\n",
    "    NO_COUNTERPART = 0\n",
    "    SMALLER_TB__MATERIALIZED_CP = 1\n",
    "    SMALLER_TB__UNMATERIALIZED_CP = 2\n",
    "    LARGER_TB__MATERIALIZED_CP = 3\n",
    "    LARGER_TB__UNMATERIALIZED_CP = 4\n",
    "\n",
    "\n",
    "class MaterializedFieldFromSameTableCategory(IntEnum):\n",
    "    \"\"\"Categories for whether there is a materialized field from the same table\"\"\"\n",
    "    NONE = 0,\n",
    "    OTHER = 1,\n",
    "    SELECT = 2,\n",
    "    SELECT_NON_NULL = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_size(column_name):\n",
    "    \"\"\"Get the table size for a given column based on its prefix\"\"\"\n",
    "    if column_name.startswith(\"c_\"):\n",
    "        return TPCH_TABLE_SIZES[\"customer\"]\n",
    "    elif column_name.startswith(\"l_\"):\n",
    "        return TPCH_TABLE_SIZES[\"lineitem\"]\n",
    "    elif column_name.startswith(\"o_\"):\n",
    "        return TPCH_TABLE_SIZES[\"orders\"]\n",
    "    elif column_name.startswith(\"p_\"):\n",
    "        return TPCH_TABLE_SIZES[\"part\"]\n",
    "    elif column_name.startswith(\"ps_\"):\n",
    "        return TPCH_TABLE_SIZES[\"partsupp\"]\n",
    "    elif column_name.startswith(\"s_\"):\n",
    "        return TPCH_TABLE_SIZES[\"supplier\"]\n",
    "    elif column_name.startswith(\"n_\"):\n",
    "        return TPCH_TABLE_SIZES[\"nation\"]\n",
    "    elif column_name.startswith(\"r_\"):\n",
    "        return TPCH_TABLE_SIZES[\"region\"]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_table_name(column_name):\n",
    "    \"\"\"Get the table name for a given column based on its prefix\"\"\"\n",
    "    if column_name.startswith(\"c_\"):\n",
    "        return \"customer\"\n",
    "    elif column_name.startswith(\"l_\"):\n",
    "        return \"lineitem\"\n",
    "    elif column_name.startswith(\"o_\"):\n",
    "        return \"orders\"\n",
    "    elif column_name.startswith(\"p_\"):\n",
    "        return \"part\"\n",
    "    elif column_name.startswith(\"ps_\"):\n",
    "        return \"partsupp\"\n",
    "    elif column_name.startswith(\"s_\"):\n",
    "        return \"supplier\"\n",
    "    elif column_name.startswith(\"n_\"):\n",
    "        return \"nation\"\n",
    "    elif column_name.startswith(\"r_\"):\n",
    "        return \"region\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Adds table size, relative improvement, and Column type\n",
    "def load_results_data():\n",
    "    \"\"\"Load and preprocess the experiment results data\"\"\"\n",
    "    results_df = pd.read_csv(RESULTS_PATH + \"expanded_results.csv\")\n",
    "\n",
    "    # Parse previous materializations into list\n",
    "    results_df[\"Previous Materializations\"] = results_df[\"Previous Materializations\"].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "    )\n",
    "\n",
    "    # Clean up any unnamed columns\n",
    "    if \"Unnamed: 0\" in results_df.columns:\n",
    "        results_df = results_df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "    # Add table size information\n",
    "    results_df[\"Table size\"] = results_df.apply(\n",
    "        lambda row: get_table_size(row[\"Materialization\"]), axis=1)\n",
    "\n",
    "    # Calculate relative improvement\n",
    "    results_df[\"Relative Improvement\"] = results_df.apply(\n",
    "        lambda row: row[\"Improvement\"] / row[\"Previous Time\"], axis=1\n",
    "    )\n",
    "\n",
    "    # Add column type information\n",
    "    results_df[\"Column Type\"] = results_df.apply(\n",
    "        lambda row: tpch_setup.COLUMN_MAP[row[\"Materialization\"]][\"type\"]\n",
    "        if row[\"Materialization\"] in tpch_setup.COLUMN_MAP else \"Unknown\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def filter_out_s_nation_key(df):\n",
    "    return df[(df['Materialization'] != 's_nationkey') & (~df['Previous Materializations'].apply(lambda x: 's_nationkey' in x))]\n",
    "\n",
    "\n",
    "# Functions for analyzing query usage patterns\n",
    "\n",
    "def get_field_frequency_for_query(query_name, materialized_column):\n",
    "    \"\"\"Count total occurrences of a column in a query\"\"\"\n",
    "    q = QUERIES[query_name]\n",
    "    cols = q.columns_used()\n",
    "    return cols.count(materialized_column)\n",
    "\n",
    "\n",
    "def get_field_join_frequency_for_query(query_name, materialized_column):\n",
    "    \"\"\"Count how many times a column is used in joins\"\"\"\n",
    "    q = QUERIES[query_name]\n",
    "    cols = q.columns_used_in_join()\n",
    "    if materialized_column in cols:\n",
    "        return len(cols[materialized_column])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_field_where_frequency_for_query(query_name, materialized_column):\n",
    "    \"\"\"Count how many times a column is used in WHERE clauses\"\"\"\n",
    "    q = QUERIES[query_name]\n",
    "    cols = q.columns_used_with_position()[\"where\"]\n",
    "    return cols.count(materialized_column)\n",
    "\n",
    "\n",
    "def get_field_select_frequency_for_query(query_name, materialized_column):\n",
    "    \"\"\"Count how many times a column is used in SELECT clauses\"\"\"\n",
    "    q = QUERIES[query_name]\n",
    "    cols = q.columns_used_with_position()[\"select\"]\n",
    "    return cols.count(materialized_column)\n",
    "\n",
    "\n",
    "def get_field_group_by_frequency_for_query(query_name, materialized_column):\n",
    "    \"\"\"Count how many times a column is used in GROUP BY clauses\"\"\"\n",
    "    q = QUERIES[query_name]\n",
    "    cols = q.columns_used_with_position()[\"group_by\"]\n",
    "    return cols.count(materialized_column)\n",
    "\n",
    "\n",
    "def get_field_order_by_frequency_for_query(query_name, materialized_column):\n",
    "    \"\"\"Count how many times a column is used in ORDER BY clauses\"\"\"\n",
    "    q = QUERIES[query_name]\n",
    "    cols = q.columns_used_with_position()[\"order_by\"]\n",
    "    return cols.count(materialized_column)\n",
    "\n",
    "\n",
    "def get_self_join_frequency_for_query(query_name, materialized_column):\n",
    "    \"\"\"Count how many times a column is used in self-joins\"\"\"\n",
    "    q = QUERIES[query_name]\n",
    "    cols = q.columns_used_with_position()\n",
    "    if \"self_join\" in cols and materialized_column in cols[\"self_join\"].keys():\n",
    "        return cols[\"self_join\"][materialized_column]\n",
    "    return 0\n",
    "\n",
    "# Join utility functions\n",
    "\n",
    "def get_join_category(query_name, materialized_column, previous_materializations, table_size):\n",
    "    \"\"\"Determine the join category for a materialized column\"\"\"\n",
    "    q = QUERIES[query_name]\n",
    "\n",
    "    # Check if the column is used in a join\n",
    "    if materialized_column not in q.columns_used_in_join():\n",
    "        assert False\n",
    "        return JoinCategory.NO_COUNTERPART\n",
    "\n",
    "    counterpart = q.columns_used_in_join()[materialized_column][0]\n",
    "    if counterpart is None:\n",
    "        return JoinCategory.NO_COUNTERPART\n",
    "\n",
    "    is_counterpart_materialized = counterpart in previous_materializations\n",
    "\n",
    "    table_size_materialized = table_size\n",
    "    table_size_counterpart = get_table_size(counterpart)\n",
    "\n",
    "    if table_size_materialized > table_size_counterpart:\n",
    "        return JoinCategory.LARGER_TB__MATERIALIZED_CP if is_counterpart_materialized else JoinCategory.LARGER_TB__UNMATERIALIZED_CP\n",
    "    elif table_size_materialized < table_size_counterpart:\n",
    "        return JoinCategory.SMALLER_TB__MATERIALIZED_CP if is_counterpart_materialized else JoinCategory.SMALLER_TB__UNMATERIALIZED_CP\n",
    "    else:\n",
    "        # Equal size - default to smaller table category\n",
    "        return JoinCategory.SMALLER_TB__MATERIALIZED_CP if is_counterpart_materialized else JoinCategory.SMALLER_TB__UNMATERIALIZED_CP\n",
    "    \n",
    "\n",
    "def get_is_field_from_same_table_materialized(query_name, materialized_column, previous_materializations):\n",
    "    \"\"\"Check if there is a materialized field from the same table\"\"\"\n",
    "    q = QUERIES[query_name]\n",
    "\n",
    "    if materialized_column not in q.columns_used():\n",
    "        assert False\n",
    "\n",
    "    for column in previous_materializations:\n",
    "        if get_table_name(column) == get_table_name(materialized_column):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_join_counterpart_info(query_name, materialized_column, previous_materializations):\n",
    "    \"\"\"Get information about the join counterpart of a materialized column\"\"\"\n",
    "    q = QUERIES[query_name]\n",
    "\n",
    "    if materialized_column not in q.columns_used_in_join():\n",
    "        return None, None, False\n",
    "\n",
    "    counterpart = q.columns_used_in_join()[materialized_column][0]\n",
    "    if counterpart is None:\n",
    "        return None, None, False\n",
    "\n",
    "    counterpart_table = get_table_name(counterpart)\n",
    "    counterpart_size = get_table_size(counterpart)\n",
    "    is_materialized = counterpart in previous_materializations\n",
    "\n",
    "    return counterpart_table, counterpart_size, is_materialized\n",
    "\n",
    "def get_is_filtering_on_same_table(query_name, materialized_column):\n",
    "    q = QUERIES[query_name]\n",
    "    return q.join_field_has_filter(materialized_column)\n",
    "\n",
    "def get_is_table_used_in_multiple_joins(query_name, materialized_column):\n",
    "    q = QUERIES[query_name]\n",
    "\n",
    "    materialized_column_table_name = get_table_name(materialized_column)\n",
    "\n",
    "    join_fields = q.columns_used_in_join()\n",
    "\n",
    "    for join_field in join_fields:\n",
    "        if join_field == materialized_column:\n",
    "            continue\n",
    "        if get_table_name(join_field) == materialized_column_table_name:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Where utility functions\n",
    "\n",
    "def get_is_direct_filter(query_name, materialized_column):\n",
    "    q = QUERIES[query_name]\n",
    "    return q.where_field_has_direct_filter(materialized_column)\n",
    "\n",
    "\n",
    "def get_is_previous_materialized_field_on_same_table_and_direct_filter(query_name, materialized_column, previous_materializations):\n",
    "    q = QUERIES[query_name]\n",
    "    for column in previous_materializations:\n",
    "        if get_table_name(column) == get_table_name(materialized_column):\n",
    "            if column in q.columns_used_in_where():\n",
    "                if q.where_field_has_direct_filter(column):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Data preparation functions\n",
    "\n",
    "def add_usage_frequencies(df):\n",
    "    \"\"\"Add column usage frequency information to the dataframe\"\"\"\n",
    "    df[\"Total Frequency\"] = df.apply(\n",
    "        lambda row: get_field_frequency_for_query(row[\"Query\"], row[\"Materialization\"]), axis=1\n",
    "    )\n",
    "\n",
    "    df[\"Join Frequency\"] = df.apply(\n",
    "        lambda row: get_field_join_frequency_for_query(row[\"Query\"], row[\"Materialization\"]), axis=1\n",
    "    )\n",
    "\n",
    "    df[\"Where Frequency\"] = df.apply(\n",
    "        lambda row: get_field_where_frequency_for_query(row[\"Query\"], row[\"Materialization\"]), axis=1\n",
    "    )\n",
    "\n",
    "    df[\"Select Frequency\"] = df.apply(\n",
    "        lambda row: get_field_select_frequency_for_query(row[\"Query\"], row[\"Materialization\"]), axis=1\n",
    "    )\n",
    "\n",
    "    df[\"Group By Frequency\"] = df.apply(\n",
    "        lambda row: get_field_group_by_frequency_for_query(row[\"Query\"], row[\"Materialization\"]), axis=1\n",
    "    )\n",
    "\n",
    "    df[\"Order By Frequency\"] = df.apply(\n",
    "        lambda row: get_field_order_by_frequency_for_query(row[\"Query\"], row[\"Materialization\"]), axis=1\n",
    "    )\n",
    "\n",
    "    df[\"Self Join Frequency\"] = df.apply(\n",
    "        lambda row: get_self_join_frequency_for_query(row[\"Query\"], row[\"Materialization\"]), axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_rankings(df):\n",
    "    \"\"\"Add ranking information based on improvement\"\"\"\n",
    "\n",
    "    # Global rankings\n",
    "    df[\"Global Rank\"] = df[\"Improvement\"].rank(ascending=False).astype(int)\n",
    "    df[\"Global Percentile\"] = df[\"Improvement\"].rank(pct=True).round(2)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_is_field_from_same_table_materialized_flag(df):\n",
    "    # Add materialized field from same table category\n",
    "    df[\"Is Field From Same Table Materialized\"] = df.apply(\n",
    "        lambda row: get_is_field_from_same_table_materialized(\n",
    "            row[\"Query\"],\n",
    "            row[\"Materialization\"],\n",
    "            row[\"Previous Materializations\"]\n",
    "            ),\n",
    "            axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def round_improvement_outliers(df):\n",
    "    \"\"\"Round improvement outliers to the nearest 100\"\"\"\n",
    "    df[\"Improvement\"] = df[\"Improvement\"].apply(\n",
    "        lambda x: 1.5 if x > 1.5 else (-0.25 if x < -0.25 else x)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_single_join_data(df):\n",
    "    \"\"\"Extract and prepare data for single join analysis\"\"\"\n",
    "    # Filter for single join cases\n",
    "    single_join_df = df[(df[\"Join Frequency\"] == 1) &\n",
    "                        (df[\"Where Frequency\"] == 0)].copy()\n",
    "\n",
    "    # Add join-specific rankings\n",
    "    single_join_df[\"Global Join Rank\"] = single_join_df[\"Improvement\"].rank(\n",
    "        ascending=False)\n",
    "    single_join_df[\"Global Join Percentile\"] = single_join_df[\"Improvement\"].rank(\n",
    "        pct=True).round(2)\n",
    "\n",
    "    # Add join category information\n",
    "    single_join_df[\"Join Category\"] = single_join_df.apply(\n",
    "        lambda row: get_join_category(\n",
    "            row[\"Query\"],\n",
    "            row[\"Materialization\"],\n",
    "            row[\"Previous Materializations\"],\n",
    "            row[\"Table size\"]\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Add counterpart information\n",
    "    join_counterparts = single_join_df.apply(\n",
    "        lambda row: get_join_counterpart_info(\n",
    "            row[\"Query\"],\n",
    "            row[\"Materialization\"],\n",
    "            row[\"Previous Materializations\"]\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    single_join_df[\"Join Counterpart Table Name\"] = join_counterparts.apply(\n",
    "        lambda x: x[0] if x else None)\n",
    "    single_join_df[\"Join Counterpart Table Size\"] = join_counterparts.apply(\n",
    "        lambda x: x[1] if x else None)\n",
    "    single_join_df[\"Join Counterpart Is Materialized\"] = join_counterparts.apply(\n",
    "        lambda x: x[2] if x else False)\n",
    "    \n",
    "\n",
    "    # Add same table filter flag\n",
    "    single_join_df[\"Is Filtering On Same Table\"] = single_join_df.apply(\n",
    "        lambda row: get_is_filtering_on_same_table(\n",
    "            row[\"Query\"],\n",
    "            row[\"Materialization\"]),\n",
    "            axis=1\n",
    "            )\n",
    "    \n",
    "    single_join_df[\"Is Table Used In Multiple Joins\"] = single_join_df.apply(\n",
    "        lambda row: get_is_table_used_in_multiple_joins(\n",
    "            row[\"Query\"],\n",
    "            row[\"Materialization\"]),\n",
    "            axis=1\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    return single_join_df\n",
    "\n",
    "\n",
    "def prepare_single_where_data(df):\n",
    "    \"\"\"Extract and prepare data for single WHERE clause analysis\"\"\"\n",
    "    # Filter for single WHERE cases\n",
    "    single_where_df = df[(df[\"Where Frequency\"] == 1) &\n",
    "                         (df[\"Join Frequency\"] == 0)].copy()\n",
    "\n",
    "    # Add WHERE-specific rankings\n",
    "    single_where_df[\"Global Where Percentile\"] = single_where_df[\"Improvement\"].rank(\n",
    "        pct=True)\n",
    "\n",
    "    # Add direct filter flag\n",
    "    single_where_df[\"Is Direct Filter\"] = single_where_df.apply(\n",
    "        lambda row: get_is_direct_filter(\n",
    "            row[\"Query\"],\n",
    "            row[\"Materialization\"]\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Previous materialized field is on same table and direct where\n",
    "    single_where_df[\"Is Previous Materialized Field On Same Table And Direct Where\"] = single_where_df.apply(\n",
    "        lambda row: get_is_previous_materialized_field_on_same_table_and_direct_filter(\n",
    "            row[\"Query\"],\n",
    "            row[\"Materialization\"],\n",
    "            row[\"Previous Materializations\"]\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    return single_where_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "\n",
    "def plot_scatter(x, y, title, xlabel, ylabel, colorby=None, cmap='tab20', alpha=0.5, figsize=(12, 8)):\n",
    "    \"\"\"Create a scatter plot with optional color coding\"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    if colorby is not None:\n",
    "        categories = colorby.astype('category').cat.codes\n",
    "        scatter = plt.scatter(x, y, c=categories, alpha=alpha, cmap=cmap)\n",
    "\n",
    "        # Add colorbar\n",
    "        colorbar = plt.colorbar()\n",
    "        colorbar.set_ticks(range(len(colorby.unique())))\n",
    "        colorbar.set_ticklabels(sorted(colorby.unique()))\n",
    "    else:\n",
    "        scatter = plt.scatter(x, y, alpha=alpha)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Add grid\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, plt.gca()\n",
    "\n",
    "\n",
    "def plot_violin(data_dict, title, xlabel, ylabel, figsize=(12, 8), showmeans=True):\n",
    "    \"\"\"Create a violin plot from a dictionary of data\"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    # Convert dict values to list\n",
    "    data_values = list(data_dict.values())\n",
    "\n",
    "    # Create violin plot\n",
    "    violin_plot = plt.violinplot(data_values, showmeans=showmeans)\n",
    "\n",
    "    # Set x-axis ticks and labels\n",
    "    plt.xticks(\n",
    "        range(1, len(data_dict) + 1),\n",
    "        list(data_dict.keys()),\n",
    "        rotation=45,\n",
    "        ha='right'\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Add counts above each violin\n",
    "    ax = plt.gca()\n",
    "    try:\n",
    "        max_height = max([max(v) if len(v) > 0 else 0 for v in data_values])\n",
    "        # Get the current y-axis limits\n",
    "        ymin, ymax = ax.get_ylim()\n",
    "        # Place text just below the upper limit\n",
    "        text_y = ymax - (ymax - ymin) * 0.02\n",
    "        for i, (key, values) in enumerate(data_dict.items(), 1):\n",
    "            ax.text(i, text_y,\n",
    "                    f'n={len(values)}', ha='center', va='top')\n",
    "    except ValueError:\n",
    "        # Handle empty data\n",
    "        pass\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, plt.gca()\n",
    "\n",
    "\n",
    "def plot_bar(x, y, title, xlabel, ylabel, figsize=(15, 6), annotate=None, rotation=45):\n",
    "    \"\"\"Create a bar plot with optional annotation\"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    bars = plt.bar(x, y)\n",
    "\n",
    "    # Set x-axis labels\n",
    "    plt.xticks(rotation=rotation, ha='right')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Add annotations if provided\n",
    "    if annotate is not None:\n",
    "        ax = plt.gca()\n",
    "        max_height = max(y)\n",
    "        for i, (bar, ann) in enumerate(zip(bars, annotate)):\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2.,\n",
    "                bar.get_height() + max_height * 0.02,\n",
    "                f'n={ann}', ha='center', va='bottom'\n",
    "            )\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, plt.gca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_for_frequency_combination(row):\n",
    "    \"\"\"Convert frequency columns to a readable label\"\"\"\n",
    "    freqs = [row[\"Join Frequency\"], row[\"Where Frequency\"],\n",
    "             row[\"Select Frequency\"], row[\"Group By Frequency\"]]\n",
    "    labels = [\"Join\", \"Where\", \"Select\", \"Group By\"]\n",
    "    active = [l for f, l in zip(freqs, labels) if f == 1]\n",
    "    return \", \".join(active) if active else \"Other\"\n",
    "\n",
    "def get_simplified_category(row):\n",
    "    \"\"\"Convert to simplified categories: Join, Where, Join and Where, or Other\"\"\"\n",
    "    join_freq = row[\"Join Frequency\"]\n",
    "    where_freq = row[\"Where Frequency\"]\n",
    "\n",
    "    if join_freq == 1 and where_freq == 0:\n",
    "        return \"Join\"\n",
    "    elif join_freq == 0 and where_freq == 1:\n",
    "        return \"Where\"\n",
    "    elif join_freq == 1 and where_freq == 1:\n",
    "        return \"Join and Where\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def binarize_frequencies(df, cols):\n",
    "    \"\"\"Binarize frequency columns by setting any frequency greater than 0 to 1\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in cols:\n",
    "        df_copy[f\"{col} Frequency\"] = df_copy[f\"{col} Frequency\"].apply(\n",
    "            lambda x: int(x > 0))\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def group_and_sum(df, group_cols, sum_col, sum_name, count_name):\n",
    "    \"\"\"Group dataframe by specified columns and compute count and sum of another column\"\"\"\n",
    "    grouped_count = df.groupby(group_cols).size().reset_index(name=count_name)\n",
    "    grouped_sum = df.groupby(group_cols).agg(\n",
    "        **{sum_name: (sum_col, 'sum')}).reset_index()\n",
    "    return grouped_count.merge(grouped_sum, on=group_cols, how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis functions\n",
    "\n",
    "def analyze_frequency_patterns(df, percentile_threshold=PERCENTILE_THRESHOLD):\n",
    "    \"\"\"Analyze how column usage patterns affect improvement percentiles\"\"\"\n",
    "    # Create filtered dataframes\n",
    "    filtered_global = df[df[\"Global Percentile\"] >= percentile_threshold]\n",
    "\n",
    "    # Add column with readable label for frequency combination\n",
    "    df[\"Label\"] = df.apply(get_label_for_frequency_combination, axis=1)\n",
    "\n",
    "    # Group by frequency combination and count occurrences\n",
    "    freq_cols = [\"Join Frequency\", \"Where Frequency\",\n",
    "                 \"Select Frequency\", \"Group By Frequency\"]\n",
    "\n",
    "    # Process for global percentile\n",
    "    filtered_counts_global = (\n",
    "        filtered_global\n",
    "        .groupby(freq_cols)\n",
    "        .size()\n",
    "        .reset_index(name='FilteredCount')\n",
    "    )\n",
    "\n",
    "    # Get total counts\n",
    "    total_counts = (\n",
    "        df\n",
    "        .groupby(freq_cols)\n",
    "        .size()\n",
    "        .reset_index(name='TotalCount')\n",
    "    )\n",
    "\n",
    "    # Merge and compute normalized counts for global percentile\n",
    "    counts_global = pd.merge(\n",
    "        filtered_counts_global,\n",
    "        total_counts,\n",
    "        on=freq_cols,\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    counts_global['Label'] = counts_global.apply(\n",
    "        lambda row: get_label_for_frequency_combination(row),\n",
    "        axis=1\n",
    "    )\n",
    "    counts_global['Normalized'] = counts_global['FilteredCount'] / \\\n",
    "        counts_global['TotalCount']\n",
    "    counts_global = counts_global.sort_values('FilteredCount', ascending=False)\n",
    "\n",
    "    return counts_global\n",
    "\n",
    "\n",
    "def analyze_join_patterns(single_join_df, percentile_threshold=PERCENTILE_THRESHOLD):\n",
    "    \"\"\"Analyze how join patterns affect improvement in single join cases\"\"\"\n",
    "    # Filter data based on percentile threshold\n",
    "    high_global_joins = single_join_df[single_join_df['Global Join Percentile']\n",
    "                                       >= percentile_threshold]\n",
    "\n",
    "    # Group by join category and count\n",
    "\n",
    "    gj_counts = high_global_joins.groupby(\n",
    "        'Join Category').size().reset_index(name='FilteredCount')\n",
    "    gj_counts['Sum'] = high_global_joins.groupby(\n",
    "        'Join Category')['Improvement'].sum().values\n",
    "\n",
    "    # Get total counts by category\n",
    "    total_counts = single_join_df.groupby(\n",
    "        'Join Category').size().reset_index(name='TotalCount')\n",
    "\n",
    "    # Prepare results\n",
    "\n",
    "    gj_results = pd.merge(gj_counts, total_counts,\n",
    "                          on='Join Category', how='inner')\n",
    "    gj_results['Normalized'] = gj_results['FilteredCount'] / \\\n",
    "        gj_results['TotalCount']\n",
    "\n",
    "    return gj_results\n",
    "\n",
    "\n",
    "def analyze_where_patterns(single_where_df, percentile_threshold=PERCENTILE_THRESHOLD):\n",
    "    \"\"\"Analyze how WHERE clause patterns affect improvement\"\"\"\n",
    "    # Filter data based on percentile threshold\n",
    "    high_global_where = single_where_df[single_where_df['Global Where Percentile']\n",
    "                                        >= percentile_threshold]\n",
    "\n",
    "    # Analyze by column type\n",
    "    col_type_global = high_global_where.groupby(\n",
    "        'Column Type').size().reset_index(name='Count')\n",
    "    col_type_global['Sum'] = high_global_where.groupby(\n",
    "        'Column Type')['Improvement'].sum().values\n",
    "\n",
    "    # Analyze by table size\n",
    "\n",
    "    table_size_global = high_global_where.groupby(\n",
    "        'Table size').size().reset_index(name='Count')\n",
    "    table_size_global['Sum'] = high_global_where.groupby(\n",
    "        'Table size')['Improvement'].sum().values\n",
    "\n",
    "    return {\n",
    "        'column_type': {'global': col_type_global},\n",
    "        'table_size': {'global': table_size_global}\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_binarized_frequencies(df, percentile_threshold=PERCENTILE_THRESHOLD):\n",
    "    \"\"\"Analyze binary usage patterns (e.g., whether a column is used in a clause at all)\"\"\"\n",
    "    # Define frequency columns to binarize\n",
    "    frequency_cols = [\"Join\", \"Where\", \"Select\", \"Group By\"]\n",
    "\n",
    "    # Create filtered dataframes\n",
    "    filtered_global = df[df[\"Global Percentile\"] >= percentile_threshold]\n",
    "\n",
    "    # Binarize frequencies (0/1 instead of actual counts)\n",
    "    binary_df = binarize_frequencies(df, frequency_cols)\n",
    "    binary_filtered_global = binarize_frequencies(\n",
    "        filtered_global, frequency_cols)\n",
    "\n",
    "    # Add labels\n",
    "    binary_df[\"Label\"] = binary_df.apply(\n",
    "        get_label_for_frequency_combination, axis=1)\n",
    "\n",
    "       # Add simplified category labels\n",
    "    binary_df[\"Simplified Category\"] = binary_df.apply(\n",
    "        get_simplified_category, axis=1)\n",
    "\n",
    "    # Define grouping columns\n",
    "    group_cols = [f\"{col} Frequency\" for col in frequency_cols]\n",
    "\n",
    "\n",
    "    # Process global percentile data\n",
    "    total_grouped = group_and_sum(\n",
    "        binary_df,\n",
    "        group_cols=group_cols,\n",
    "        sum_col=\"Improvement\",\n",
    "        sum_name=\"TotalSum\",\n",
    "        count_name=\"Count\"\n",
    "    )\n",
    "\n",
    "    filtered_grouped_global = group_and_sum(\n",
    "        binary_filtered_global,\n",
    "        group_cols=group_cols,\n",
    "        sum_col=\"Improvement\",\n",
    "        sum_name=\"FilteredSum\",\n",
    "        count_name=\"FilteredCount\"\n",
    "    )\n",
    "\n",
    "    binary_counts_global = pd.merge(\n",
    "        filtered_grouped_global,\n",
    "        total_grouped,\n",
    "        on=group_cols,\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    binary_counts_global[\"Label\"] = binary_counts_global.apply(\n",
    "        get_label_for_frequency_combination, axis=1)\n",
    "    binary_counts_global[\"Normalized\"] = binary_counts_global[\"FilteredCount\"] / \\\n",
    "        binary_counts_global[\"Count\"]\n",
    "    binary_counts_global = binary_counts_global.sort_values(\n",
    "        'FilteredCount', ascending=False)\n",
    "\n",
    "    # TODO: Fix order\n",
    "    # Map to original order\n",
    "    # binary_counts_global[\"sort_order\"] = binary_counts_global[\"Label\"].map(\n",
    "    #     {label: i for i, label in enumerate(binary_counts_query[\"Label\"])}\n",
    "    # )\n",
    "    # binary_counts_global = binary_counts_global.sort_values('sort_order')\n",
    "\n",
    "    return binary_df, binary_counts_global\n",
    "\n",
    "\n",
    "\n",
    "def analyze_join_category_by_table_size(single_join_df):\n",
    "    \"\"\"Analyze join categories for each table size\"\"\"\n",
    "    # Create violin plot data for Global percentiles by join category\n",
    "    join_category_data = {}\n",
    "\n",
    "    # Get all table sizes\n",
    "    table_sizes = {\n",
    "        5: 'region (5 rows)',\n",
    "        25: 'nation (25 rows)',\n",
    "        5000: 'supplier (5000 rows)',\n",
    "        75000: 'customer (75000 rows)',\n",
    "        100000: 'part (100000 rows)',\n",
    "        400000: 'partsupp (400000 rows)',\n",
    "        750000: 'orders (750000 rows)',\n",
    "        3000000: 'lineitem (3000000 rows)'\n",
    "    }\n",
    "\n",
    "    # Category labels\n",
    "    category_labels = {\n",
    "        JoinCategory.NO_COUNTERPART: 'No Counterpart',\n",
    "        JoinCategory.SMALLER_TB__MATERIALIZED_CP: 'Smaller Table Materialized CP',\n",
    "        JoinCategory.SMALLER_TB__UNMATERIALIZED_CP: 'Smaller Table Unmaterialized CP',\n",
    "        JoinCategory.LARGER_TB__MATERIALIZED_CP: 'Larger Table Materialized CP',\n",
    "        JoinCategory.LARGER_TB__UNMATERIALIZED_CP: 'Larger Table Unmaterialized CP'\n",
    "    }\n",
    "\n",
    "    # Prepare data for each table size\n",
    "    for size, label in table_sizes.items():\n",
    "        join_category_data[size] = {}\n",
    "        join_category_data[size]['global'] = {}\n",
    "        join_category_data[size]['counts'] = {}\n",
    "\n",
    "        # Get data for each join category within this table size\n",
    "        for cat in JoinCategory:\n",
    "            df_subset = single_join_df[(single_join_df['Table size'] == size) &\n",
    "                                       (single_join_df['Join Category'] == cat)]\n",
    "\n",
    "            if len(df_subset) > 0:\n",
    "                cat_label = category_labels[cat]\n",
    "                join_category_data[size]['global'][cat_label] = df_subset['Global Join Percentile']\n",
    "                join_category_data[size]['counts'][cat_label] = len(df_subset)\n",
    "\n",
    "    return join_category_data, table_sizes, category_labels\n",
    "\n",
    "\n",
    "def analyze_join_counterparts(single_join_df):\n",
    "    \"\"\"Analyze join counterparts for each table size\"\"\"\n",
    "    counterpart_data = {}\n",
    "\n",
    "    # Get all table sizes\n",
    "    table_sizes = {\n",
    "        5: 'region (5 rows)',\n",
    "        25: 'nation (25 rows)',\n",
    "        5000: 'supplier (5000 rows)',\n",
    "        75000: 'customer (75000 rows)',\n",
    "        100000: 'part (100000 rows)',\n",
    "        400000: 'partsupp (400000 rows)',\n",
    "        750000: 'orders (750000 rows)',\n",
    "        3000000: 'lineitem (3000000 rows)'\n",
    "    }\n",
    "\n",
    "    # Prepare data for each table size\n",
    "    for size, label in table_sizes.items():\n",
    "        counterpart_data[size] = {}\n",
    "        counterpart_data[size]['global'] = {}\n",
    "        counterpart_data[size]['counts'] = {}\n",
    "\n",
    "        # Get data for this table size\n",
    "        df_subset = single_join_df[single_join_df['Table size'] == size]\n",
    "\n",
    "        if len(df_subset) > 0:\n",
    "            # Group by counterpart table\n",
    "            for cp_name, cp_size in df_subset[['Join Counterpart Table Name', 'Join Counterpart Table Size']].dropna().drop_duplicates().sort_values('Join Counterpart Table Size', ascending=False).values:\n",
    "                # Get data for materialized counterpart\n",
    "                mat_data = df_subset[\n",
    "                    (df_subset['Join Counterpart Table Name'] == cp_name) &\n",
    "                    (df_subset['Join Counterpart Is Materialized'] == True)\n",
    "                ]\n",
    "\n",
    "                # Get data for unmaterialized counterpart\n",
    "                unmat_data = df_subset[\n",
    "                    (df_subset['Join Counterpart Table Name'] == cp_name) &\n",
    "                    (df_subset['Join Counterpart Is Materialized'] == False)\n",
    "                ]\n",
    "\n",
    "                if len(mat_data) > 0:\n",
    "                    label = f'{cp_name}\\n{int(cp_size)}\\nMat.'\n",
    "                    counterpart_data[size]['global'][label] = mat_data['Global Join Percentile']\n",
    "                    counterpart_data[size]['counts'][label] = len(mat_data)\n",
    "\n",
    "                if len(unmat_data) > 0:\n",
    "                    label = f'{cp_name}\\n{int(cp_size)}\\nUnmat.'\n",
    "                    counterpart_data[size]['global'][label] = unmat_data['Global Join Percentile']\n",
    "                    counterpart_data[size]['counts'][label] = len(unmat_data)\n",
    "\n",
    "    return counterpart_data, table_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "\n",
    "#Adds table size, relative improvement, and Column type\n",
    "results_df = load_results_data()\n",
    "\n",
    "results_df = add_usage_frequencies(results_df)\n",
    "\n",
    "#Adds both query and global\n",
    "results_df = add_rankings(results_df)\n",
    "\n",
    "#Adds flag for whether the field is from the same table as the materialization\n",
    "results_df = add_is_field_from_same_table_materialized_flag(results_df)\n",
    "\n",
    "#Rounds improvement outliers to 1.5 and -0.25\n",
    "results_df = round_improvement_outliers(results_df)\n",
    "\n",
    "\n",
    "# Optional: Filter out evert row with s_nation as materialization or in previous materializations\n",
    "# results_df = filter_out_s_nation_key(results_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filt_results_df = results_df[(results_df['Query'] == 'q18') & (results_df['Materialization'] == 'p_brand') & (results_df.apply(lambda row: \"p_size\" in row[\"Previous Materializations\"], axis=1))]\n",
    "filt_results_df = results_df[(results_df['Query'] == 'q16') & (results_df['Materialization'] == 'p_brand') & (results_df.apply(lambda row: \"p_size\" in row[\"Previous Materializations\"], axis=1))]\n",
    "filt_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_where_df = prepare_single_where_data(filt_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_where_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare specialized datasets\n",
    "single_join_df = prepare_single_join_data(results_df)\n",
    "single_where_df = prepare_single_where_data(results_df)\n",
    "single_where_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic improvement plots\n",
    "fig, ax = plot_scatter(\n",
    "    results_df['Previous Time'],\n",
    "    results_df['Improvement'],\n",
    "    'Improvement vs Previous Execution Time',\n",
    "    'Previous Execution Time (s)',\n",
    "    'Improvement (s)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_scatter(\n",
    "    results_df['Previous Time'],\n",
    "    results_df['Relative Improvement'],\n",
    "    'Relative Improvement vs Previous Execution Time',\n",
    "    'Previous Execution Time (s)',\n",
    "    'Relative Improvement'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for negative improvements\n",
    "results_df_negative_improvement = results_df[results_df['Improvement'] < 0]\n",
    "\n",
    "if not results_df_negative_improvement.empty:\n",
    "    fig, ax = plot_scatter(\n",
    "        results_df_negative_improvement['Previous Time'],\n",
    "        results_df_negative_improvement['Improvement'],\n",
    "        'Negative Improvement vs Previous Execution Time',\n",
    "        'Previous Execution Time (s)',\n",
    "        'Improvement (s)',\n",
    "        colorby=results_df_negative_improvement['Query']\n",
    "    )\n",
    "\n",
    "\n",
    "    fig, ax = plot_scatter(\n",
    "        results_df_negative_improvement['Previous Time'],\n",
    "        results_df_negative_improvement['Improvement'],\n",
    "        'Negative Improvement vs Previous Execution Time (by Materialization)',\n",
    "        'Previous Execution Time (s)',\n",
    "        'Improvement (s)',\n",
    "        colorby=results_df_negative_improvement['Materialization']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add non-binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze frequency patterns\n",
    "counts_global = analyze_frequency_patterns(results_df)\n",
    "\n",
    "# Analyze binarized frequency patterns\n",
    "binary_df, binary_counts_global = analyze_binarized_frequencies(\n",
    "    results_df)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plot_bar(\n",
    "    binary_counts_global['Label'],\n",
    "    binary_counts_global['FilteredCount'],\n",
    "    f'Raw Counts in Filtered Data (Global Percentile ≥ {PERCENTILE_THRESHOLD})',\n",
    "    'Frequency Combination',\n",
    "    'Count'\n",
    ")\n",
    "\n",
    "fig, ax = plot_bar(\n",
    "    binary_counts_global['Label'],\n",
    "    binary_counts_global['Normalized'],\n",
    "    f'Normalized Counts (Global Percentile ≥ {PERCENTILE_THRESHOLD})',\n",
    "    'Frequency Combination',\n",
    "    'Count / Total',\n",
    "    annotate=binary_counts_global['Count']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create violin plots for binarized frequencies\n",
    "\n",
    "global_violin_data = {\n",
    "    label: binary_df[binary_df['Label'] == label]['Improvement']\n",
    "    for label in binary_counts_global['Label'].tolist()\n",
    "}\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    global_violin_data,\n",
    "    'Distribution of Global Percentiles by Category',\n",
    "    'Category',\n",
    "    'Global Percentile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create improvement sum bar plot\n",
    "fig, ax = plot_bar(\n",
    "    binary_counts_global['Label'],\n",
    "    binary_counts_global['TotalSum'],\n",
    "    'Sum of Improvement for each category (no percentile)',\n",
    "    'Frequency Combination',\n",
    "    'Sum of Improvement',\n",
    "    annotate=binary_counts_global['Count']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create violin plots for simplified categories\n",
    "simplified_categories = [\"Join\", \"Where\", \"Join and Where\", \"Other\"]\n",
    "\n",
    "global_violin_simplified = {\n",
    "    category: binary_df[binary_df['Simplified Category']\n",
    "                        == category]['Improvement']\n",
    "    for category in simplified_categories\n",
    "}\n",
    "\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    global_violin_simplified,\n",
    "    'Distribution of Improvement by Usage',\n",
    "    'Usage',\n",
    "    'Improvement (s)'\n",
    ")\n",
    "\n",
    "# Create improvement sum bar plot for simplified categories\n",
    "simplified_sum_data = binary_df.groupby('Simplified Category')[\n",
    "    'Improvement'].sum()\n",
    "simplified_count_data = binary_df.groupby('Simplified Category').size()\n",
    "\n",
    "fig, ax = plot_bar(\n",
    "    simplified_sum_data.index,\n",
    "    simplified_sum_data.values,\n",
    "    'Sum of Improvement by Simplified Category',\n",
    "    'Category',\n",
    "    'Sum of Improvement',\n",
    "    annotate=simplified_count_data.values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine single where and join dataframes with origin labels\n",
    "combined_df = pd.concat([\n",
    "    single_where_df[['Improvement']].assign(Origin='Single Where'),\n",
    "    single_join_df[['Improvement']].assign(Origin='Single Join')\n",
    "])\n",
    "\n",
    "# Create violin plot comparing distributions\n",
    "fig, ax = plot_violin(\n",
    "    {origin: group['Improvement'] for origin, group in combined_df.groupby('Origin')},\n",
    "    'Distribution of Improvement Single Where vs Single Join',\n",
    "    'Usage',\n",
    "    'Improvement (s)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Join Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze join patterns\n",
    "gj_results = analyze_join_patterns(single_join_df)\n",
    "\n",
    "# Create plots for join categories\n",
    "category_labels = [cat.name for cat in JoinCategory]\n",
    "\n",
    "\n",
    "fig, ax = plot_bar(\n",
    "    [category_labels[int(cat)] for cat in gj_results['Join Category']],\n",
    "    gj_results['FilteredCount'],\n",
    "    f'Join Categories (Global Join Percentile ≥ {PERCENTILE_THRESHOLD})',\n",
    "    'Join Category',\n",
    "    'Count',\n",
    "    annotate=gj_results['TotalCount']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create improvement sum plots for join categories\n",
    "fig, ax = plot_bar(\n",
    "    [category_labels[int(cat)] for cat in gj_results['Join Category']],\n",
    "    gj_results['Sum'],\n",
    "    f'Improvement Sum by Join Category (Global Join Percentile ≥ {PERCENTILE_THRESHOLD})',\n",
    "    'Join Category',\n",
    "    'Sum of Improvement',\n",
    "    annotate=gj_results['TotalCount']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global percentiles\n",
    "join_category_global_percentiles = {}\n",
    "for cat in JoinCategory:\n",
    "    data = single_join_df[single_join_df['Join Category']\n",
    "                            == cat]['Global Join Percentile']\n",
    "    if len(data) > 0:\n",
    "        join_category_global_percentiles[cat.name] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    join_category_global_percentiles,\n",
    "    'Single join: Distribution of Global Join percentiles by Join Category',\n",
    "    'Join Category',\n",
    "    'Global Join percentile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_filtering_on_same_table_global_percentiles = {}\n",
    "labels = {True: 'Filter', False: 'No Filter'}\n",
    "for is_filtering_on_same_table in [True, False]:\n",
    "    data = single_join_df[single_join_df['Is Filtering On Same Table']\n",
    "                            == is_filtering_on_same_table]['Improvement']\n",
    "    if len(data) > 0:\n",
    "        is_filtering_on_same_table_global_percentiles[labels[is_filtering_on_same_table]] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    is_filtering_on_same_table_global_percentiles,\n",
    "    'Single join: Distribution of Improvement by Is Filtering On Same Table',\n",
    "    'Is Filtering On Same Table',\n",
    "    'Improvement (s)'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_in_stet = len(single_join_df[(single_join_df[\"Improvement\"] < 0.25) & (single_join_df[\"Is Filtering On Same Table\"] == False)])\n",
    "number_in_stet_where_table_used_in_multiple_joins = len(single_join_df[(single_join_df[\"Improvement\"] < 0.25) & (single_join_df[\"Is Filtering On Same Table\"] == False) & (single_join_df[\"Is Table Used In Multiple Joins\"] == True)])\n",
    "print(f\"Number in stet: {number_in_stet}\")\n",
    "print(f\"Number in stet where table used in multiple joins: {number_in_stet_where_table_used_in_multiple_joins}\")\n",
    "print(f\"Proportion: {round(number_in_stet_where_table_used_in_multiple_joins / number_in_stet, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "is_filtering_on_same_table_global_percentiles = {}\n",
    "join_categories = sorted(single_join_df['Join Category'].unique())\n",
    "\n",
    "category_names = {\n",
    "    0: \"No Counterpart\",\n",
    "    1: \"Smaller Table, Materialized Counterpart\", \n",
    "    2: \"Smaller Table, Unmaterialized Counterpart\",\n",
    "    3: \"Larger Table, Materialized Counterpart\",\n",
    "    4: \"Larger Table, Unmaterialized Counterpart\"\n",
    "}\n",
    "\n",
    "for join_category in join_categories:\n",
    "    data = single_join_df[\n",
    "        (single_join_df['Is Filtering On Same Table'] == False) \n",
    "        & (single_join_df['Join Category'] == join_category) \n",
    "    ]['Improvement']\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        is_filtering_on_same_table_global_percentiles[category_names[join_category]] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    is_filtering_on_same_table_global_percentiles,\n",
    "    'Single join: Distribution of Improvement by Table Size (No Filter)',\n",
    "    'Table Size',''\n",
    "    'Improvement (s)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "is_filtering_on_same_table_global_percentiles = {}\n",
    "join_categories = sorted(single_join_df['Join Category'].unique())\n",
    "\n",
    "category_names = {\n",
    "    0: \"No Counterpart\",\n",
    "    1: \"Smaller Table, Materialized Counterpart\", \n",
    "    2: \"Smaller Table, Unmaterialized Counterpart\",\n",
    "    3: \"Larger Table, Materialized Counterpart\",\n",
    "    4: \"Larger Table, Unmaterialized Counterpart\"\n",
    "}\n",
    "\n",
    "for join_category in join_categories:\n",
    "    data = single_join_df[\n",
    "        (single_join_df['Is Filtering On Same Table'] == False) \n",
    "        & (single_join_df['Join Category'] == join_category) \n",
    "        & (single_join_df['Improvement'] > 0.3)\n",
    "    ]['Improvement']\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        is_filtering_on_same_table_global_percentiles[category_names[join_category]] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    is_filtering_on_same_table_global_percentiles,\n",
    "    'Single join: Distribution of Improvement by Table Size (No Filter)',\n",
    "    'Table Size',''\n",
    "    'Improvement (s)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_filtering_on_same_table_global_percentiles = {}\n",
    "table_sizes = sorted(single_join_df['Table size'].unique())\n",
    "\n",
    "for table_size in table_sizes:\n",
    "    data = single_join_df[\n",
    "        (single_join_df['Is Filtering On Same Table'] == False) \n",
    "        & (single_join_df['Table size'] == table_size) \n",
    "    ]['Improvement']\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        is_filtering_on_same_table_global_percentiles[f'Table size: {table_size}'] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    is_filtering_on_same_table_global_percentiles,\n",
    "    'Single join: Distribution of Improvement by Table Size (No Filter)',\n",
    "    'Table Size',''\n",
    "    'Improvement (s)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "is_filtering_on_same_table_global_percentiles = {}\n",
    "table_sizes = sorted(single_join_df['Table size'].unique())\n",
    "\n",
    "for table_size in table_sizes:\n",
    "    data = single_join_df[\n",
    "        (single_join_df['Is Filtering On Same Table'] == False) \n",
    "        & (single_join_df['Table size'] == table_size) \n",
    "        & (single_join_df[\"Improvement\"] > 0.4) \n",
    "        & (single_join_df[\"Improvement\"] < 1)\n",
    "    ]['Improvement']\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        is_filtering_on_same_table_global_percentiles[f'Table size: {table_size}'] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    is_filtering_on_same_table_global_percentiles,\n",
    "    'Single join: Distribution of Improvement by Table Size (No Filter)',\n",
    "    'Table Size',''\n",
    "    'Improvement (s)'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for cases with no filtering on same table\n",
    "no_filter_df = single_join_df[single_join_df[\"Is Filtering On Same Table\"] == False]\n",
    "# no_filter_df = single_join_df[single_join_df[\"Improvement\"] > 0.3]\n",
    "\n",
    "\n",
    "# Create scatter plot of improvement vs previous execution time\n",
    "fig, ax = plot_scatter(\n",
    "    x=no_filter_df[\"Previous Time\"],\n",
    "    y=no_filter_df[\"Improvement\"], \n",
    "    title=\"Improvement vs Previous Time (No Filter)\",\n",
    "    xlabel=\"Previous Time (s)\",\n",
    "    ylabel=\"Improvement (s)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_filtering_on_same_table_global_percentiles = {}\n",
    "labels = {\n",
    "    (False, True): 'No Filter + Table in Multiple Joins', \n",
    "    (False, False): 'No Filter + Table in Single Join'\n",
    "}\n",
    "\n",
    "is_filtering_on_same_table = False\n",
    "for is_table_used_in_multiple_joins in [True, False]:\n",
    "    data = single_join_df[\n",
    "        (single_join_df['Is Filtering On Same Table'] == is_filtering_on_same_table) &\n",
    "        (single_join_df['Is Table Used In Multiple Joins'] == is_table_used_in_multiple_joins)\n",
    "    ]['Improvement']\n",
    "    if len(data) > 0:\n",
    "        is_filtering_on_same_table_global_percentiles[labels[(is_filtering_on_same_table, is_table_used_in_multiple_joins)]] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    is_filtering_on_same_table_global_percentiles,\n",
    "    'Single join: Distribution of Improvement by Filter and Join Usage',\n",
    "    'Filter and Join Usage Category',\n",
    "    'Improvement (s)'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_filtering_on_same_table_global_percentiles = {}\n",
    "labels = {\n",
    "    (True, True): 'Filter + Same Table Mat.',\n",
    "    (True, False): 'Filter',\n",
    "    (False, True): 'No Filter + Same Table Mat.',\n",
    "    (False, False): 'No Filter'\n",
    "}\n",
    "\n",
    "for is_filtering_on_same_table in [True, False]:\n",
    "    for is_field_from_same_table in [True, False]:\n",
    "        data = single_join_df[\n",
    "            (single_join_df['Is Filtering On Same Table'] == is_filtering_on_same_table) &\n",
    "            (single_join_df['Is Field From Same Table Materialized'] == is_field_from_same_table)\n",
    "        ]['Improvement']\n",
    "        if len(data) > 0:\n",
    "            is_filtering_on_same_table_global_percentiles[labels[(is_filtering_on_same_table, is_field_from_same_table)]] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    is_filtering_on_same_table_global_percentiles,\n",
    "    'Single join: Distribution of Improvement by Filter and Materialization Location',\n",
    "    'Filter and Materialization Category',\n",
    "    'Improvement (s)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global join percentiles by table size\n",
    "table_size_global_percentiles = {\n",
    "    'region (5 rows)': single_join_df[single_join_df['Table size'] == 5]['Global Join Percentile'],\n",
    "    'nation (25 rows)': single_join_df[single_join_df['Table size'] == 25]['Global Join Percentile'],\n",
    "    'supplier (5000 rows)': single_join_df[single_join_df['Table size'] == 5000]['Global Join Percentile'],\n",
    "    'customer (75000 rows)': single_join_df[single_join_df['Table size'] == 75000]['Global Join Percentile'],\n",
    "    'part (100000 rows)': single_join_df[single_join_df['Table size'] == 100000]['Global Join Percentile'],\n",
    "    'partsupp (400000 rows)': single_join_df[single_join_df['Table size'] == 400000]['Global Join Percentile'],\n",
    "    'orders (750000 rows)': single_join_df[single_join_df['Table size'] == 750000]['Global Join Percentile'],\n",
    "    'lineitem (3000000 rows)': single_join_df[single_join_df['Table size'] == 3000000]['Global Join Percentile']\n",
    "}\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    table_size_global_percentiles,\n",
    "    'Single join: Distribution of Global Join percentiles by Table Size',\n",
    "    'Table Size',\n",
    "    'Global Join percentile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single join improvement scatter plot\n",
    "fig, ax = plot_scatter(\n",
    "    single_join_df['Previous Time'],\n",
    "    single_join_df['Improvement'],\n",
    "    'Single join: Improvement vs Previous Execution Time',\n",
    "    'Previous Execution Time (s)',\n",
    "    'Improvement (s)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analyze join categories by table size\n",
    "# join_category_data, table_sizes, category_labels = analyze_join_category_by_table_size(\n",
    "#     single_join_df)\n",
    "\n",
    "# # Create violin plots for each table size\n",
    "# for size, label in table_sizes.items():\n",
    "#     if size in join_category_data:\n",
    "\n",
    "#         fig, ax = plot_violin(\n",
    "#             join_category_data[size]['global'],\n",
    "#             f'Distribution of Global Join Percentiles for {label}',\n",
    "#             'Join Category',\n",
    "#             'Global Join Percentile'\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analyze join counterparts\n",
    "# counterpart_data, table_sizes = analyze_join_counterparts(single_join_df)\n",
    "\n",
    "# # Create violin plots for each table size's counterparts\n",
    "# for size, label in table_sizes.items():\n",
    "#     if size in counterpart_data:\n",
    "#         fig, ax = plot_violin(\n",
    "#             counterpart_data[size]['global'],\n",
    "#             f'Distribution of Global Join Percentiles for {label} by Counterpart',\n",
    "#             'Join Counterpart',\n",
    "#             'Global Join Percentile'\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create violin plots for table sizes\n",
    "global_table_sizes = {\n",
    "    'region (5 rows)': results_df[results_df['Table size'] == 5]['Global Percentile'],\n",
    "    'nation (25 rows)': results_df[results_df['Table size'] == 25]['Global Percentile'],\n",
    "    'supplier (5000 rows)': results_df[results_df['Table size'] == 5000]['Global Percentile'],\n",
    "    'customer (75000 rows)': results_df[results_df['Table size'] == 75000]['Global Percentile'],\n",
    "    'part (100000 rows)': results_df[results_df['Table size'] == 100000]['Global Percentile'],\n",
    "    'partsupp (400000 rows)': results_df[results_df['Table size'] == 400000]['Global Percentile'],\n",
    "    'orders (750000 rows)': results_df[results_df['Table size'] == 750000]['Global Percentile'],\n",
    "    'lineitem (3000000 rows)': results_df[results_df['Table size'] == 3000000]['Global Percentile']\n",
    "}\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    global_table_sizes,\n",
    "    'Distribution of Global Percentiles by Table Size',\n",
    "    'Table Size',\n",
    "    'Global Percentile'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Where Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global WHERE percentiles by column type\n",
    "where_column_types_global = {}\n",
    "for ct in single_where_df['Column Type'].unique():\n",
    "    data = single_where_df[single_where_df['Column Type']\n",
    "                            == ct]['Global Where Percentile']\n",
    "    if len(data) > 0:\n",
    "        where_column_types_global[ct] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    where_column_types_global,\n",
    "    'Single Where: Distribution of Global Where Percentiles by Column Type',\n",
    "    'Column Type',\n",
    "    'Global Where Percentile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "is_direct_filter_global_percentiles = {}\n",
    "labels = {True: 'Direct Filter', False: 'Not Direct Filter'}\n",
    "for is_direct_filter in [True, False]:\n",
    "    data = single_where_df[single_where_df['Is Direct Filter'] == is_direct_filter]['Improvement']\n",
    "    if len(data) > 0:\n",
    "        is_direct_filter_global_percentiles[labels[is_direct_filter]] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    is_direct_filter_global_percentiles,\n",
    "    'Single Where: Distribution of Improvement by Is Direct Filter',\n",
    "    'Is Direct Filter',\n",
    "    'Improvement (s)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_filter_df = single_where_df[single_where_df['Is Direct Filter'] == True]\n",
    "\n",
    "is_direct_filter_global_percentiles = {}\n",
    "labels = {True: 'Direct filter + Previous Field Direct Where', False: 'Direct filter + No Previous Field Direct Where'}\n",
    "for prev_direct_where in [True, False]:\n",
    "    data = direct_filter_df[direct_filter_df['Is Previous Materialized Field On Same Table And Direct Where'] == prev_direct_where]['Improvement']\n",
    "    if len(data) > 0:\n",
    "        is_direct_filter_global_percentiles[labels[prev_direct_where]] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    is_direct_filter_global_percentiles,\n",
    "    'Single Where (Direct Filter Only): Distribution of Improvement by Previous Field Direct Where',\n",
    "    'Previous Field Direct Where Status',\n",
    "    'Improvement (s)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_filter_df = single_where_df[single_where_df['Is Direct Filter'] == True]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_filter_df = single_where_df[single_where_df['Is Direct Filter'] == True]\n",
    "\n",
    "is_direct_filter_global_percentiles = {}\n",
    "for ct in direct_filter_df['Column Type'].unique():\n",
    "    data = direct_filter_df[(direct_filter_df['Column Type'] == ct) & (direct_filter_df[\"Improvement\"] < 0.9)]['Improvement']\n",
    "    if len(data) > 0:\n",
    "        is_direct_filter_global_percentiles[ct] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    is_direct_filter_global_percentiles,\n",
    "    'Single Where (Direct Filter Only): Distribution of Improvement by Column Type',\n",
    "    'Column Type',\n",
    "    'Improvement (s)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_filter_df = single_where_df[single_where_df['Is Direct Filter'] == True]\n",
    "\n",
    "table_sizes = sorted(single_join_df['Table size'].unique())\n",
    "\n",
    "is_direct_filter_global_percentiles = {}\n",
    "for table_size in table_sizes:\n",
    "    data = direct_filter_df[(direct_filter_df['Table size'] == table_size) & (direct_filter_df[\"Improvement\"] > 0.0) & (direct_filter_df[\"Improvement\"] < 0.9)]['Improvement']\n",
    "    if len(data) > 0:\n",
    "        is_direct_filter_global_percentiles[table_size] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    is_direct_filter_global_percentiles,\n",
    "    'Single Where (Direct Filter Only): Distribution of Improvement by Table size',\n",
    "    'Table size',\n",
    "    'Improvement (s)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_filter_df = single_where_df[single_where_df['Is Direct Filter'] == True]\n",
    "\n",
    "table_sizes = sorted(single_join_df['Table size'].unique())\n",
    "\n",
    "is_direct_filter_global_percentiles = {}\n",
    "for table_size in table_sizes:\n",
    "    data = direct_filter_df[(direct_filter_df['Table size'] == table_size) & (direct_filter_df[\"Improvement\"] > 0.5) & (direct_filter_df[\"Improvement\"] < 0.9)]['Improvement']\n",
    "    if len(data) > 0:\n",
    "        is_direct_filter_global_percentiles[table_size] = data\n",
    "\n",
    "fig, ax = plot_violin(\n",
    "    is_direct_filter_global_percentiles,\n",
    "    'Single Where (Direct Filter Only): Distribution of Improvement by Table size',\n",
    "    'Table size',\n",
    "    'Improvement (s)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scatter plot of improvement vs previous time for direct filter\n",
    "fig, ax = plot_scatter(\n",
    "    direct_filter_df['Previous Time'],\n",
    "    direct_filter_df['Improvement'],\n",
    "    'Single Where (Direct Filter Only): Improvement vs Previous Execution Time',\n",
    "    'Previous Execution Time (s)',\n",
    "    'Improvement (s)'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# single_where_df_field_from_same_table_not_materialized = single_where_df[single_where_df['Is Field From Same Table Materialized'] == False].copy()\n",
    "\n",
    "# is_direct_filter_global_percentiles = {}\n",
    "# for is_direct_filter in [True, False]:\n",
    "#     data = single_where_df_field_from_same_table_not_materialized[single_where_df_field_from_same_table_not_materialized['Is Direct Filter'] == is_direct_filter]['Improvement']\n",
    "#     if len(data) > 0:\n",
    "#         is_direct_filter_global_percentiles[is_direct_filter] = data\n",
    "\n",
    "# fig, ax = plot_violin(\n",
    "#     is_direct_filter_global_percentiles,\n",
    "#     '(SINGLE WHERE) Distribution of Improvement by Is Direct Filter',\n",
    "#     'Is Direct Filter',\n",
    "#     'Improvement (s)'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# single_where_df_field_from_same_table_materialized = single_where_df[single_where_df['Is Field From Same Table Materialized'] == True].copy()\n",
    "\n",
    "# is_direct_filter_global_percentiles = {}\n",
    "# for is_direct_filter in [True, False]:\n",
    "#     data = single_where_df_field_from_same_table_materialized[single_where_df_field_from_same_table_materialized['Is Direct Filter'] == is_direct_filter]['Improvement']\n",
    "#     if len(data) > 0:\n",
    "#         is_direct_filter_global_percentiles[is_direct_filter] = data\n",
    "\n",
    "# fig, ax = plot_violin(\n",
    "#     is_direct_filter_global_percentiles,\n",
    "#     '(SINGLE WHERE) Distribution of Improvement by Is Direct Filter',\n",
    "#     'Is Direct Filter',\n",
    "#     'Improvement (s)'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO plots\n",
    "\n",
    "\n",
    "### Backlog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
